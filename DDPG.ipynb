{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d28e813-5310-4015-bb52-fedc9867fbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: (5, 5, 2, 2, 1, 1, -1, -1, 3, 0, -1, -1, 4, 4, 2, 3, 4, 0, 2, 4, 1, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: [0.7215400323407826, 0.22876222127045265, 0.9452706955539223, 0.9014274576114836, 0.030589983033553536, 0.0254458609934608, 0.5414124727934966, 0.9391491627785106, 0.38120423768821243, 0.21659939713061338]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from MazeEnv import MazeEnv\n",
    "\n",
    "maze = MazeEnv(5, 5)\n",
    "print(\"State:\", maze.get_state())\n",
    "maze.draw_maze()\n",
    "action = maze.random_action()\n",
    "maze.step(action)\n",
    "print(\"Action:\", action)\n",
    "maze.draw_maze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f57b712c-d90f-4300-bc9c-891ea7b4d489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练参数如下：\n",
      "================================================================================\n",
      "        参数名         \t        参数值         \t        参数类型        \n",
      "     algo_name      \t        DDPG        \t   <class 'str'>    \n",
      "      env_name      \t    Pendulum-v1     \t   <class 'str'>    \n",
      "     train_eps      \t        300         \t   <class 'int'>    \n",
      "      test_eps      \t         1          \t   <class 'int'>    \n",
      "     max_steps      \t        100         \t   <class 'int'>    \n",
      "       gamma        \t        0.99        \t  <class 'float'>   \n",
      "     critic_lr      \t       0.001        \t  <class 'float'>   \n",
      "      actor_lr      \t       0.0001       \t  <class 'float'>   \n",
      "  memory_capacity   \t        8000        \t   <class 'int'>    \n",
      "     batch_size     \t        128         \t   <class 'int'>    \n",
      "   target_update    \t         2          \t   <class 'int'>    \n",
      "        tau         \t        0.01        \t  <class 'float'>   \n",
      " critic_hidden_dim  \t        256         \t   <class 'int'>    \n",
      "  actor_hidden_dim  \t        256         \t   <class 'int'>    \n",
      "       device       \t        cpu         \t   <class 'str'>    \n",
      "        seed        \t         1          \t   <class 'int'>    \n",
      "================================================================================\n",
      "开始训练！\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 3183.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成训练！\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from MazeEnv import MazeEnv\n",
    "\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, n_states, n_actions, hidden_dim=256, init_w=3e-3):\n",
    "        super(Actor, self).__init__()\n",
    "        self.linear1 = nn.Linear(n_states, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, n_actions)\n",
    "\n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = torch.tanh(self.linear3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, n_states, n_actions, hidden_dim=256, init_w=3e-3):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(n_states + n_actions, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, 1)\n",
    "        # 随机初始化为较小的值\n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        # 按维数1拼接\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "    \n",
    "class DDPG:\n",
    "    def __init__(self, models, memories, cfg):\n",
    "        self.device = torch.device(cfg['device'])\n",
    "        self.critic = models['critic'].to(self.device)\n",
    "        self.target_critic = models['critic'].to(self.device)\n",
    "        self.actor = models['actor'].to(self.device)\n",
    "        self.target_actor = models['actor'].to(self.device)\n",
    "\n",
    "        # 复制参数到目标网络\n",
    "        for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        for target_param, param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        self.critic_optimizer = optim.Adam(\n",
    "            self.critic.parameters(),  lr=cfg['critic_lr'])\n",
    "        self.actor_optimizer = optim.Adam(\n",
    "            self.actor.parameters(), lr=cfg['actor_lr'])\n",
    "        self.memory = memories['memory']\n",
    "        self.batch_size = cfg['batch_size']\n",
    "        self.gamma = cfg['gamma']\n",
    "        self.tau = cfg['tau']  # 软更新参数\n",
    "\n",
    "    def sample_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        action = self.actor(state)\n",
    "        return action.detach().cpu().numpy()[0]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_action(self, state):\n",
    "        ''' 用于预测，不需要计算梯度\n",
    "        '''\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        action = self.actor(state)\n",
    "        print(\"predict_action\",action)\n",
    "        return action.cpu().numpy()[0]\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.memory) < self.batch_size:  # 当memory中不满足一个批量时，不更新策略\n",
    "            return\n",
    "        # 从经验回放中中随机采样一个批量的transition\n",
    "        state, action, reward, next_state, done = self.memory.sample(\n",
    "            self.batch_size)\n",
    "        # 转变为张量\n",
    "        state = torch.FloatTensor(np.array(state)).to(self.device)\n",
    "        next_state = torch.FloatTensor(np.array(next_state)).to(self.device)\n",
    "        action = torch.FloatTensor(np.array(action)).to(self.device)\n",
    "        reward = torch.FloatTensor(reward).unsqueeze(1).to(self.device)\n",
    "        done = torch.FloatTensor(np.float32(done)).unsqueeze(1).to(self.device)\n",
    "        # 注意看伪代码，这里的actor损失就是对应策略即actor输出的action下对应critic值的负均值\n",
    "        actor_loss = self.critic(state, self.actor(state))\n",
    "        actor_loss = - actor_loss.mean()\n",
    "\n",
    "        next_action = self.target_actor(next_state)\n",
    "        target_value = self.target_critic(next_state, next_action.detach())\n",
    "        # 这里的expected_value就是伪代码中间的y_i\n",
    "        expected_value = reward + (1.0 - done) * self.gamma * target_value\n",
    "        expected_value = torch.clamp(expected_value, -np.inf, np.inf)\n",
    "\n",
    "        actual_value = self.critic(state, action)\n",
    "        critic_loss = nn.MSELoss()(actual_value, expected_value.detach())\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        # 各自目标网络的参数软更新\n",
    "        for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - self.tau) +\n",
    "                param.data * self.tau\n",
    "            )\n",
    "        for target_param, param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - self.tau) +\n",
    "                param.data * self.tau\n",
    "                )\n",
    "            \n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=self.capacity)\n",
    "\n",
    "    def push(self, transitions):\n",
    "        '''_summary_\n",
    "        Args:\n",
    "            trainsitions (tuple): _description_\n",
    "        '''\n",
    "        self.buffer.append(transitions)\n",
    "\n",
    "    def sample(self, batch_size: int, sequential: bool = False):\n",
    "        if batch_size > len(self.buffer):\n",
    "            batch_size = len(self.buffer)\n",
    "        if sequential:  # sequential sampling\n",
    "            rand = random.randint(0, len(self.buffer) - batch_size)\n",
    "            batch = [self.buffer[i] for i in range(rand, rand + batch_size)]\n",
    "            return zip(*batch)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "            return zip(*batch)\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "def all_seed(env, seed=1):\n",
    "    ''' 万能的seed函数\n",
    "    '''\n",
    "    # env.seed(seed)  # env config\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)  # config for CPU\n",
    "    torch.cuda.manual_seed(seed)  # config for GPU\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)  # config for python scripts\n",
    "    # config for cudnn\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    \n",
    "def env_agent_config(cfg):\n",
    "#     env = NormalizedActions(gym.make(cfg['env_name']))  # 装饰action噪声\n",
    "    env = MazeEnv(10,10)\n",
    "    if cfg['seed'] != 0:\n",
    "        all_seed(env, seed=cfg['seed'])\n",
    "    n_states = env.get_n_states()\n",
    "    n_actions = env.get_n_actions()\n",
    "    # env.draw_maze()\n",
    "#     n_states = env.observation_space.shape[0]\n",
    "#     n_actions = env.action_space.shape[0]\n",
    "    # 更新n_states和n_actions到cfg参数中\n",
    "    cfg.update({\"n_states\": n_states, \"n_actions\": n_actions})\n",
    "    models = {\"actor\": Actor(n_states, n_actions, hidden_dim=cfg['actor_hidden_dim']), \"critic\": Critic(\n",
    "        n_states, n_actions, hidden_dim=cfg['critic_hidden_dim'])}\n",
    "    memories = {\"memory\": ReplayBuffer(cfg['memory_capacity'])}\n",
    "    agent = DDPG(models, memories, cfg)\n",
    "    return env, agent\n",
    "\n",
    "def get_args():\n",
    "    \"\"\" 超参数\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"hyperparameters\")\n",
    "    parser.add_argument('--algo_name', default='DDPG',\n",
    "                        type=str, help=\"name of algorithm\")\n",
    "    parser.add_argument('--env_name', default='Pendulum-v1',\n",
    "                        type=str, help=\"name of environment\")\n",
    "    parser.add_argument('--train_eps', default=300,\n",
    "                        type=int, help=\"episodes of training\")\n",
    "    parser.add_argument('--test_eps', default=1, type=int,\n",
    "                        help=\"episodes of testing\")\n",
    "    parser.add_argument('--max_steps', default=100, type=int,\n",
    "                        help=\"steps per episode, much larger value can simulate infinite steps\")\n",
    "    parser.add_argument('--gamma', default=0.99,\n",
    "                        type=float, help=\"discounted factor\")\n",
    "    parser.add_argument('--critic_lr', default=1e-3,\n",
    "                        type=float, help=\"learning rate of critic\")\n",
    "    parser.add_argument('--actor_lr', default=1e-4,\n",
    "                        type=float, help=\"learning rate of actor\")\n",
    "    parser.add_argument('--memory_capacity', default=8000,\n",
    "                        type=int, help=\"memory capacity\")\n",
    "    parser.add_argument('--batch_size', default=128, type=int)\n",
    "    parser.add_argument('--target_update', default=2, type=int)\n",
    "    parser.add_argument('--tau', default=1e-2, type=float)\n",
    "    parser.add_argument('--critic_hidden_dim', default=256, type=int)\n",
    "    parser.add_argument('--actor_hidden_dim', default=256, type=int)\n",
    "    parser.add_argument('--device', default='cpu',\n",
    "                        type=str, help=\"cpu or cuda\")\n",
    "    parser.add_argument('--seed', default=1, type=int, help=\"random seed\")\n",
    "    args = parser.parse_args([])\n",
    "    args = {**vars(args)}  # 将args转换为字典\n",
    "    # 打印参数\n",
    "    print(\"训练参数如下：\")\n",
    "    print(''.join(['=']*80))\n",
    "    tplt = \"{:^20}\\t{:^20}\\t{:^20}\"\n",
    "    print(tplt.format(\"参数名\", \"参数值\", \"参数类型\"))\n",
    "    for k, v in args.items():\n",
    "        print(tplt.format(k, v, str(type(v))))\n",
    "    print(''.join(['=']*80))\n",
    "    return args\n",
    "\n",
    "def smooth(data, weight=0.9):\n",
    "    '''用于平滑曲线，类似于Tensorboard中的smooth\n",
    "\n",
    "    Args:\n",
    "        data (List):输入数据\n",
    "        weight (Float): 平滑权重，处于0-1之间，数值越高说明越平滑，一般取0.9\n",
    "\n",
    "    Returns:\n",
    "        smoothed (List): 平滑后的数据\n",
    "    '''\n",
    "    last = data[0]  # First value in the plot (first timestep)\n",
    "    smoothed = list()\n",
    "    for point in data:\n",
    "        smoothed_val = last * weight + (1 - weight) * point  # 计算平滑值\n",
    "        smoothed.append(smoothed_val)\n",
    "        last = smoothed_val\n",
    "    return smoothed\n",
    "\n",
    "\n",
    "def plot_rewards(rewards, cfg, path=None, tag='train'):\n",
    "    sns.set()\n",
    "    plt.figure()  # 创建一个图形实例，方便同时多画几个图\n",
    "    plt.title(\n",
    "        f\"{tag}ing curve on {cfg['device']} of {cfg['algo_name']} for {cfg['env_name']}\")\n",
    "    plt.xlabel('epsiodes')\n",
    "    plt.plot(rewards, label='rewards')\n",
    "    plt.plot(smooth(rewards), label='smoothed')\n",
    "    plt.legend()\n",
    "\n",
    "class OUNoise(object):\n",
    "    '''Ornstein–Uhlenbeck噪声\n",
    "    '''\n",
    "    def __init__(self, n_actions, mu=0.0, theta=0.15, max_sigma=0.3, min_sigma=0.3, decay_period=100000):\n",
    "        self.mu           = mu # OU噪声的参数\n",
    "        self.theta        = theta # OU噪声的参数\n",
    "        self.sigma        = max_sigma # OU噪声的参数\n",
    "        self.max_sigma    = max_sigma\n",
    "        self.min_sigma    = min_sigma\n",
    "        self.decay_period = decay_period\n",
    "        self.n_actions   = n_actions\n",
    "        self.low          = 0\n",
    "        self.high         = 1\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.obs = np.ones(self.n_actions) * self.mu\n",
    "    \n",
    "    def evolve_obs(self):\n",
    "        x  = self.obs\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.n_actions)\n",
    "        self.obs = x + dx\n",
    "        return self.obs\n",
    "    \n",
    "    def get_action(self, action, t=0):\n",
    "        ou_obs = self.evolve_obs()\n",
    "        self.sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_period) # sigma会逐渐衰减\n",
    "        return np.clip(action + ou_obs, self.low, self.high) # 动作加上噪声后进行剪切\n",
    "\n",
    "def train(cfg, env, agent):\n",
    "    print(\"开始训练！\")\n",
    "    ou_noise = OUNoise(env.get_n_actions())  # 动作噪声\n",
    "    rewards = [] # 记录所有回合的奖励\n",
    "    for i_ep in range(cfg['train_eps']):\n",
    "        state = env.reset()\n",
    "        ou_noise.reset()\n",
    "        ep_reward = 0\n",
    "        for i_step in tqdm(range(cfg['max_steps'])):\n",
    "            action = agent.sample_action(state)\n",
    "            action = ou_noise.get_action(action, i_step+1)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            ep_reward += reward\n",
    "            agent.memory.push((state, action, reward, next_state, done))\n",
    "            agent.update()\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        if (i_ep+1)%10 == 0:\n",
    "            print(f\"回合：{i_ep+1}/{cfg['train_eps']}，奖励：{ep_reward:.2f}\")\n",
    "        rewards.append(ep_reward)\n",
    "    print(\"完成训练！\")\n",
    "    return {'rewards':rewards}\n",
    "\n",
    "def test(cfg, env, agent):\n",
    "    print(\"开始测试！\")\n",
    "    rewards = [] # 记录所有回合的奖励\n",
    "    for i_ep in range(cfg['test_eps']):\n",
    "        state = env.reset() \n",
    "        ep_reward = 0\n",
    "        for i_step in range(cfg['max_steps']):\n",
    "            action = agent.predict_action(state)\n",
    "            print(action)\n",
    "            env.draw_maze()\n",
    "            next_state, reward, done = env.step(action)\n",
    "            ep_reward += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        rewards.append(ep_reward)\n",
    "        print(f\"回合：{i_ep+1}/{cfg['test_eps']}，奖励：{ep_reward:.2f}\")\n",
    "    print(\"完成测试！\")\n",
    "    return {'rewards':rewards}\n",
    "\n",
    "# 获取参数\n",
    "cfg = get_args()\n",
    "cfg['max_steps'] = 100\n",
    "cfg['train_eps'] = 1\n",
    "cfg['test_eps'] = 1\n",
    "# 训练\n",
    "env, agent = env_agent_config(cfg)\n",
    "res_dic = train(cfg, env, agent)\n",
    "\n",
    "# plot_rewards(res_dic['rewards'], cfg, tag=\"train\")\n",
    "# # 测试\n",
    "# res_dic = test(cfg, env, agent)\n",
    "# plot_rewards(res_dic['rewards'], cfg, tag=\"test\")  # 画出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92027d7-d417-488a-a742-10552bafc7ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
