{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-today",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://blog.csdn.net/fangchenglia/article/details/125725093\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from MazeEnv import MazeEnv\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "# env = gym.make('Pendulum-v1').unwrapped\n",
    "env = MazeEnv(5, 5)\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "\n",
    "'''Pendulum环境状态特征是三个，杆子的sin(角度)、cos（角度）、角速度，（状态是无限多个，因为连续），动作值是力矩，限定在[-2,2]之间的任意的小数，所以是连续的（动作也是无限个）'''\n",
    "# state_number=env.observation_space.shape[0]\n",
    "# action_number=env.action_space.shape[0]\n",
    "# max_action = env.action_space.high[0]\n",
    "# min_action = env.action_space.low[0]\n",
    "\n",
    "state_number = env.get_n_states()\n",
    "action_number = env.get_n_actions()\n",
    "max_action = 0.99999\n",
    "min_action = -0.99999\n",
    "\n",
    "torch.manual_seed(0)#如果你觉得定了随机种子不能表达代码的泛化能力，你可以把这两行注释掉\n",
    "# env.seed(0) \n",
    "RENDER=False\n",
    "ITER_MAX = 200\n",
    "EP_MAX = 500\n",
    "EP_LEN = 40\n",
    "GAMMA = 0.99\n",
    "A_LR = 0.0001\n",
    "C_LR = 0.0003\n",
    "BATCH = 128\n",
    "A_UPDATE_STEPS = 10\n",
    "C_UPDATE_STEPS = 10\n",
    "METHOD = [\n",
    "    dict(name='kl_pen', kl_target=0.01, lam=0.5),   # KL penalty\n",
    "    dict(name='clip', epsilon=0.2),                 # Clipped surrogate objective, find this is better\n",
    "][1]        # choose the method for optimization\n",
    "Switch=0\n",
    "'''由于PPO也是基于A-C框架，所以我把PPO的编写分为两部分，PPO的第一部分 Actor'''\n",
    "'''PPO的第一步  编写A-C框架的网络，先编写actor部分的actor网络，actor的网络有新与老两个网络'''\n",
    "\n",
    "class ActorNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=256):\n",
    "        super(ActorNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3_1 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.fc3_2 = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))        \n",
    "        mean=torch.tanh(self.fc3_1(x))#输出概率分布的均值mean-1~1   TODO\n",
    "        std=F.softplus(self.fc3_2(x))#softplus激活函数的值域>0\n",
    "        return mean, std\n",
    "    \n",
    "class CriticNet(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim,hidden_dim=256):\n",
    "        super(CriticNet,self).__init__()\n",
    "        assert output_dim == 1 # critic must output a single value\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        value = self.fc3(x)\n",
    "        return value \n",
    "    \n",
    "class Actor():\n",
    "    def __init__(self):\n",
    "        self.old_pi,self.new_pi=ActorNet(state_number,action_number).to(DEVICE),ActorNet(state_number,action_number).to(DEVICE)#这只是均值mean\n",
    "        self.optimizer=torch.optim.Adam(self.new_pi.parameters(),lr=A_LR,eps=1e-5)\n",
    "    '''第二步 编写根据状态选择动作的函数'''\n",
    "    def choose_action(self,s):\n",
    "        inputstate = torch.tensor(s, dtype=torch.float32, device=DEVICE)\n",
    "        mean,std=self.old_pi(inputstate)\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        action=dist.sample()\n",
    "        action=torch.clamp(action,min_action,max_action)\n",
    "        action_logprob=dist.log_prob(action) # \n",
    "        return action.detach().cpu().numpy(),action_logprob.detach().cpu().numpy()\n",
    "    \n",
    "    def choose_action2(self,s):\n",
    "        inputstate = torch.tensor(s, dtype=torch.float32, device=DEVICE)\n",
    "        mean,std=self.old_pi(inputstate) # TODO\n",
    "        print(mean, std)\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        action=mean\n",
    "        action=torch.clamp(action,min_action,max_action)\n",
    "        action_logprob=dist.log_prob(action)\n",
    "        return action.detach().cpu.numpy(),action_logprob.detach().cpu.numpy()\n",
    "    \n",
    "    '''第四步  actor网络有两个策略（更新old策略）————————把new策略的参数赋给old策略'''\n",
    "    def update_oldpi(self):\n",
    "        self.old_pi.load_state_dict(self.new_pi.state_dict())\n",
    "    '''第六步 编写actor网络的学习函数，采用PPO2，即OpenAI推出的clip形式公式'''\n",
    "    def learn(self,bs,ba,adv,bap): #buffer_s, buffer_a, advantage, buffer_a_logp\n",
    "        bs = torch.tensor(bs, dtype=torch.float32, device=DEVICE)\n",
    "        ba = torch.tensor(ba, dtype=torch.float32, device=DEVICE)\n",
    "        adv = torch.tensor(adv, dtype=torch.float32, device=DEVICE)\n",
    "        bap = torch.tensor(bap, dtype=torch.float32, device=DEVICE)\n",
    "        for _ in range(A_UPDATE_STEPS):\n",
    "            mean, std = self.new_pi(bs)\n",
    "            dist_new=torch.distributions.Normal(mean, std)\n",
    "            action_new_logprob=dist_new.log_prob(ba)\n",
    "            ratio=torch.exp(action_new_logprob - bap.detach())\n",
    "            surr1 = ratio * adv\n",
    "            surr2 = torch.clamp(ratio, 1 - METHOD['epsilon'], 1 + METHOD['epsilon']) * adv\n",
    "            loss = -torch.min(surr1, surr2)\n",
    "            loss=loss.mean()\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.new_pi.parameters(), 0.5)\n",
    "            self.optimizer.step()\n",
    "class Critic():\n",
    "    def __init__(self):\n",
    "        self.critic_v=CriticNet(state_number,1).to(DEVICE) #改网络输入状态，生成一个V值\n",
    "        self.optimizer = torch.optim.Adam(self.critic_v.parameters(), lr=C_LR,eps=1e-5)\n",
    "        self.lossfunc = nn.MSELoss()\n",
    "    '''第三步  编写评定动作价值的函数'''\n",
    "    def get_v(self,s):\n",
    "        inputstate = torch.tensor(s, dtype=torch.float32, device=DEVICE)\n",
    "        return self.critic_v(inputstate)\n",
    "    '''第五步  计算优势——————advantage，后面发现第五步计算出来的adv可以与第七步合为一体，所以这里的代码注释了，但是，计算优势依然算是可以单独拎出来的一个步骤'''\n",
    "    # def get_adv(self,bs,br):\n",
    "    #     reality_v=torch.FloatTensor(br)\n",
    "    #     v=self.get_v(bs)\n",
    "    #     adv=(reality_v-v).detach()\n",
    "    #     return adv\n",
    "    '''第七步  编写actor-critic的critic部分的learn函数，td-error的计算代码（V现实减去V估计就是td-error）'''\n",
    "    def learn(self,bs,br):\n",
    "        bs = torch.tensor(bs, dtype=torch.float32, device=DEVICE)\n",
    "        reality_v = torch.tensor(br, dtype=torch.float32, device=DEVICE)\n",
    "        for _ in range(C_UPDATE_STEPS):\n",
    "            v=self.get_v(bs)\n",
    "            td_e = self.lossfunc(reality_v, v)\n",
    "            self.optimizer.zero_grad()\n",
    "            td_e.backward()\n",
    "            nn.utils.clip_grad_norm_(self.critic_v.parameters(), 0.5) #是否需要需要考虑\n",
    "            self.optimizer.step()\n",
    "        return (reality_v-v).detach()\n",
    "        \n",
    "def train():\n",
    "    print('PPO2训练中...')\n",
    "    actor=Actor()\n",
    "    critic=Critic()\n",
    "    \n",
    "    all_ep_r = []\n",
    "    for iteration in range(ITER_MAX): # iteration\n",
    "        b_s, b_a, b_r,b_a_logp, d_r = [], [], [], [], []\n",
    "        reward_totle=0\n",
    "        done_totle=0\n",
    "        for episode in range(EP_MAX):\n",
    "            done = False\n",
    "            buffer_s, buffer_a, buffer_r, buffer_a_logp , discounted_r= [], [], [],[], []\n",
    "            observation = env.reset() #环境重置\n",
    "            \n",
    "            for timestep in range(EP_LEN):\n",
    "                if RENDER:\n",
    "                    env.render()\n",
    "                action,action_logprob=actor.choose_action(observation)    \n",
    "                \n",
    "                observation_, reward, done, info = env.step(action)\n",
    "                buffer_s.append(observation)\n",
    "                buffer_a.append(action)\n",
    "                buffer_r.append(reward)\n",
    "                buffer_a_logp.append(action_logprob)\n",
    "                \n",
    "                observation=observation_\n",
    "                reward_totle+=reward  \n",
    "                                \n",
    "                # if(done or (observation_ in buffer_s)):\n",
    "                if(done):\n",
    "                    # print(done_totle)\n",
    "                    done_totle += 1\n",
    "                    break\n",
    "                    \n",
    "            if(done):\n",
    "                v_observation_ = torch.tensor([1e-8])\n",
    "            else:\n",
    "                v_observation_ = critic.get_v(observation_)\n",
    "            # print(v_observation_ , reward,  GAMMA)\n",
    "            for reward in buffer_r[::-1]:\n",
    "                v_observation_ = reward + GAMMA * v_observation_\n",
    "                discounted_r.append(v_observation_.detach().cpu().numpy())   # 没有转成A\n",
    "            discounted_r.reverse()\n",
    "            b_s += buffer_s\n",
    "            b_a += buffer_a\n",
    "            b_r += buffer_r\n",
    "            b_a_logp += buffer_a_logp\n",
    "            d_r += discounted_r\n",
    "            \n",
    "        d_r = torch.tensor(d_r, dtype=torch.float32)\n",
    "        d_r = (d_r - d_r.mean()) / (d_r.std() + 1e-5) \n",
    "        bs, ba, dr, bap = np.vstack(b_s), np.vstack(b_a), np.array(d_r),np.vstack(b_a_logp)\n",
    "\n",
    "        for _ in range(10):\n",
    "            msk = np.random.rand(len(bs)) < 0.2\n",
    "            \n",
    "            advantage=critic.learn(bs[msk],dr[msk])#critic部分更新\n",
    "            actor.learn(bs[msk],ba[msk],advantage,bap[msk])#actor部分更新\n",
    "            \n",
    "        actor.update_oldpi()  # pi-new的参数赋给pi-old\n",
    "        # critic.learn(bs,br)\n",
    "            \n",
    "            \n",
    "        if iteration == 0:\n",
    "            all_ep_r.append(reward_totle)\n",
    "        else:\n",
    "#             all_ep_r.append(all_ep_r[-1] * 0.9 + reward_totle * 0.1)\n",
    "            all_ep_r.append(reward_totle)\n",
    "            \n",
    "        print(\"\\rIter: {} |rewards: {} | dones: {}\".format(iteration, round(reward_totle/EP_MAX,4), round(done_totle/EP_MAX,4)), end=\"\\n\")\n",
    "        #保存神经网络参数\n",
    "        if (iteration+1) % 10 == 0 and iteration >= 0:#保存神经网络参数\n",
    "            save_data = {'net': actor.old_pi.state_dict(), 'opt': actor.optimizer.state_dict(), 'i': episode}\n",
    "            torch.save(save_data, \"./model/PPO2_model_actor.pth\")\n",
    "            save_data = {'net': critic.critic_v.state_dict(), 'opt': critic.optimizer.state_dict(), 'i': episode}\n",
    "            torch.save(save_data, \"./model/PPO2_model_critic.pth\")\n",
    "            plt.plot(np.arange(len(all_ep_r)), all_ep_r)\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Moving averaged episode reward')\n",
    "            plt.savefig(\"reward.jpg\")\n",
    "    \n",
    "    env.close()\n",
    "    plt.plot(np.arange(len(all_ep_r)), all_ep_r)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Moving averaged episode reward')\n",
    "    plt.show()\n",
    "def test(num):\n",
    "    print('PPO2测试中...')\n",
    "    aa=Actor()\n",
    "    cc=Critic()\n",
    "    checkpoint_aa = torch.load(\"./model/PPO2_model_actor.pth\")\n",
    "    aa.old_pi.load_state_dict(checkpoint_aa['net'])\n",
    "    checkpoint_cc = torch.load(\"./model/PPO2_model_critic.pth\")\n",
    "    cc.critic_v.load_state_dict(checkpoint_cc['net'])\n",
    "    for j in range(num):\n",
    "        state = env.reset()\n",
    "        total_rewards = 0\n",
    "\n",
    "        for timestep in range(EP_LEN):\n",
    "            # env.render()\n",
    "            # env.draw_maze()\n",
    "            action, action_logprob = aa.choose_action(state)\n",
    "            # env.show_action(action)\n",
    "            new_state, reward, done, info = env.step(action)  # 执行动作\n",
    "            total_rewards += reward\n",
    "            state = new_state\n",
    "            if(done):\n",
    "                break\n",
    "        print(f\"Score：{round(total_rewards,2)}, {done}\")\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-sandwich",
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-breeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
